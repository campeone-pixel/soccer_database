{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ojo7lukxLotg"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlGQeaxoLotn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "lista_ligas = ['https://fbref.com/en/comps/12/2021-2022/2021-2022-La-Liga-Stats',\n",
        "               \"https://fbref.com/en/comps/9/2021-2022/2021-2022-Premier-League-Stats\",\n",
        "               \"https://fbref.com/en/comps/11/2021-2022/2021-2022-Serie-A-Stats\",\n",
        "               \"https://fbref.com/en/comps/13/2021-2022/2021-2022-Ligue-1-Stats\",\n",
        "               \"https://fbref.com/en/comps/20/2021-2022/2021-2022-Bundesliga-Stats\",\n",
        "               ]\n",
        "\n",
        "years = list(range(2021, 2015, -1))\n",
        "all_matches = []\n",
        "log_error = []\n",
        "log = []\n",
        "for year in years:\n",
        "    for indice,liga in enumerate(lista_ligas):\n",
        "        data = requests.get(liga)\n",
        "        soup = BeautifulSoup(data.text)\n",
        "        \n",
        "   \n",
        "        try:\n",
        "            standings_table = soup.select('table.stats_table')[0]\n",
        "        except Exception as e:\n",
        "            log_error.append(f'error:{e},{lista_ligas[indice]},{team_url}')\n",
        "            continue\n",
        "        \n",
        "\n",
        "        links = [l.get(\"href\") for l in standings_table.find_all('a')]\n",
        "        links = [l for l in links if '/squads/' in l]\n",
        "        team_urls = [f\"https://fbref.com{l}\" for l in links]\n",
        "\n",
        "\n",
        "        previous_season = soup.select(\"a.prev\")[0].get(\"href\")\n",
        "        print(lista_ligas[indice])\n",
        "        \n",
        "        time.sleep(1)\n",
        "        for team_url in team_urls:\n",
        "            print(team_url)\n",
        "            team_name = team_url.split(\"/\")[-1].replace(\"-Stats\", \"\").replace(\"-\", \" \")\n",
        "            data = requests.get(team_url)\n",
        "            time.sleep(1)\n",
        "            try:\n",
        "                matches = pd.read_html(data.text, match=\"Scores & Fixtures\")[0]\n",
        "                soup = BeautifulSoup(data.text)\n",
        "                links = [l.get(\"href\") for l in soup.find_all('a')]\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                log_error.append(f'error:{e},{lista_ligas[indice]},{team_url}')\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                links_shooting = [l for l in links if l and 'all_comps/shooting/' in l]\n",
        "                data_shooting = requests.get(f\"https://fbref.com{links_shooting[0]}\")\n",
        "                shooting = pd.read_html(data_shooting.text, match=\"Shooting\")[0]\n",
        "                shooting.columns = shooting.columns.droplevel()\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                log_error.append(f'error:{e},{lista_ligas[indice]},{team_url}')\n",
        "                continue\n",
        "            try:\n",
        "                links_passing = [l for l in links if l and 'all_comps/passing/' in l]\n",
        "                data_passing = requests.get(f\"https://fbref.com{links_passing[0]}\")\n",
        "                passing = pd.read_html(data_passing.text, match=\"Passing\")[0]\n",
        "                passing.columns = passing.columns.droplevel()\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                log_error.append(f'error:{e},{lista_ligas[indice]},{team_url}')\n",
        "                continue\n",
        "            try:\n",
        "                links_defensive = [l for l in links if l and 'all_comps/defense/' in l]\n",
        "                data_defensive = requests.get(f\"https://fbref.com{links_defensive[0]}\")\n",
        "                defensive = pd.read_html(data_defensive.text, match=\"Defensive Actions\")[0]\n",
        "                defensive.columns = defensive.columns.droplevel()\n",
        "                time.sleep(1)\n",
        "            except Exception as e:\n",
        "                log_error.append(f'error:{e},{lista_ligas[indice]},{team_url}')\n",
        "                continue\n",
        "            try:\n",
        "                team_data = matches.merge(shooting[[\"Date\", \"Sh\", \"SoT\", \"Dist\", \"FK\", \"PK\", \"PKatt\",'xG']], on=\"Date\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "            try:\n",
        "                team_data = matches.merge(passing[[\"Date\",'Cmp','Att','TotDist','PrgDist','Ast','xAG', 'xA','KP']], on=\"Date\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "            try:\n",
        "                team_data = matches.merge(defensive[[\"Date\",'Tkl',\t'TklW'\t,'Def 3rd'\t,'Mid 3rd',\t'Att 3rd','Tkl'\t,'Att',\t'Tkl%'\t,'Lost'\t,'Blocks',\t'Sh',\t'Pass'\t,'Int'\t,'Tkl+Int'\t,'Clr',\t'Err' ]], on=\"Date\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "\n",
        "            team_data[\"Season\"] = year\n",
        "            team_data[\"Team\"] = team_name\n",
        "            all_matches.append(team_data)\n",
        "            time.sleep(2)\n",
        "            pass\n",
        "\n",
        "    \n",
        "     \n",
        "        lista_ligas[indice] = f\"https://fbref.com{previous_season}\"\n",
        "        print('cambie a '+lista_ligas[indice])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OE5V9Je3Lotr"
      },
      "outputs": [],
      "source": [
        "match_df = pd.concat(all_matches)\n",
        "match_df.columns = [c.lower() for c in match_df.columns]\n",
        "\n",
        "match_df.to_csv('matches.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "0f7c5ce30e5f0d7cc845aa124c9e975c020ec21fe98e4a7c7b29adf0872a8c47"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}